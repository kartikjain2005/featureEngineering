{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ***Assignment Questions***\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "8DZinmVJIKMk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **What is a parameter**?\n",
        "\n",
        " - In machine learning, a parameter usually refers to a variable that the model learns from the training data.\n",
        "\n",
        " - Example: In linear regression, the slope (weights) and intercept (bias) are parameters.\n",
        "\n",
        "  - In logistic regression, the coefficients for each feature are parameters.\n",
        "\n",
        " - In neural networks, the weights and biases across layers are parameters.\n",
        "\n",
        " ---\n"
      ],
      "metadata": {
        "id": "_u30GaZSIYms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **What is correlation? What does negative correlation mean?**\n",
        "\n",
        "- **Correlation:-**\n",
        "\n",
        "  - Correlation is a statistical measure that explains the relationship between two variables. It indicates whether and how strongly pairs of variables are related. The value of correlation coefficient (r) always lies between -1 and +1.\n",
        "\n",
        "  - r = +1 ‚Üí Perfect positive correlation\n",
        "\n",
        "  - r = -1 ‚Üí Perfect negative correlation\n",
        "\n",
        "  - r = 0 ‚Üí No correlation\n",
        "\n",
        "- **Negative Correlation :-**\n",
        "\n",
        "    - Negative correlation means that when one variable increases, the other decreases, and vice versa. In other words, the two variables move in opposite directions.\n",
        "\n",
        "    - Example:\n",
        "\n",
        "    - Price of a product and its demand ‚Üí as price increases, demand decreases.\n",
        "\n",
        "    - Number of hours spent exercising and body fat percentage ‚Üí more exercise, less body fat.\n",
        "\n",
        "    ---"
      ],
      "metadata": {
        "id": "vlHz69YeJEcx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **Define Machine Learning. What are the main components in Machine Learning**\n",
        "\n",
        " - Machine Learning (ML) is a branch of Artificial Intelligence (AI) that enables systems to learn automatically from data and improve their performance without being explicitly programmed.\n",
        "It focuses on building algorithms that can identify patterns, make predictions, or take decisions based on input data.\n",
        "\n",
        " - Arthur Samuel (1959) defined ML as:\n",
        "\"Machine Learning is the field of study that gives computers the ability to learn without being explicitly programmed.\"\n",
        "\n",
        "**Main Components of Machine Learning**\n",
        "\n",
        "1. **Dataset (Input Data):**\n",
        "\n",
        "    - The foundation of ML.\n",
        "\n",
        "    - Data is collected from various sources (images, text, numbers, sensor data, etc.).\n",
        "\n",
        "    - It is divided into training data and testing data.\n",
        "\n",
        "2. **Features (Input Variables):**\n",
        "\n",
        "    - Features are the measurable properties or characteristics of the data.\n",
        "\n",
        "    - Example: In predicting house prices, features may include size, location, number of rooms.\n",
        "\n",
        "3. Model (Algorithm):\n",
        "\n",
        "    - A mathematical representation or function that maps input features to output.\n",
        "\n",
        "    - Example: Linear Regression, Decision Trees, Neural Networks.\n",
        "\n",
        "4. Training Process:\n",
        "\n",
        "    - The process of feeding data into the model so it can learn patterns.\n",
        "\n",
        "    - The model adjusts its parameters (like weights) to minimize error.\n",
        "\n",
        "5. Loss Function (Error Function):\n",
        "\n",
        "    - Measures how far the predicted output is from the actual result.\n",
        "\n",
        "    Example: Mean Squared Error (MSE) in regression.\n",
        "\n",
        "6. Optimizer (Learning Algorithm):\n",
        "\n",
        "     - Updates model parameters to reduce the error.\n",
        "\n",
        "     - Example: Gradient Descent, Adam Optimizer.\n",
        "\n",
        "7. Evaluation (Testing):\n",
        "\n",
        "   - After training, the model is tested on unseen data to check its accuracy and performance\n",
        "\n",
        "   ---"
      ],
      "metadata": {
        "id": "u1A30pfLJ9kQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. **How does loss value help in determining whether the model is good or not?**\n",
        " - Loss value is a measure of how well or poorly a machine learning model is performing. It is calculated by comparing the model‚Äôs predicted outputs with the actual outputs.\n",
        "\n",
        "A low loss value means the model‚Äôs predictions are close to the actual values ‚Üí the model is good.\n",
        "\n",
        "A high loss value means the predictions are far from the actual values ‚Üí the model is not good.\n",
        "\n",
        "If the loss decreases during training, it shows that the model is learning properly.\n",
        "\n",
        "If the training loss is low but the testing loss is high, it means the model is overfitting.\n",
        "\n",
        "If both training and testing losses are high, it means the model is underfitting.\n",
        "\n",
        "Comparing loss values across different models also helps to identify which model performs better\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "aQVTyP3HLtoZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. **What are continuous and categorical variables?**\n",
        "- Continuous and Categorical Variables\n",
        "\n",
        "1. Continuous Variables:\n",
        "\n",
        "  - Continuous variables are numerical variables that can take any value within a range.\n",
        "\n",
        "  - They are measurable and often involve fractions or decimals.\n",
        "\n",
        "  - Examples:\n",
        "\n",
        "  -  Height of a person (e.g., 170.5 cm)\n",
        "\n",
        "  - Temperature (e.g., 36.6¬∞C)\n",
        "\n",
        "  - Salary (e.g., 45000.75 INR)\n",
        "\n",
        "2. Categorical Variables:\n",
        "\n",
        "  - Categorical variables represent distinct categories or groups.\n",
        "\n",
        "  - They are qualitative and usually cannot be measured numerically.\n",
        "\n",
        "  - Examples:\n",
        "\n",
        "  - Gender (Male, Female, Other)\n",
        "\n",
        "  - Blood Group (A, B, AB, O)\n",
        "\n",
        "  - Product Category (Electronics, Clothing, Furniture)\n",
        "\n",
        "  ---"
      ],
      "metadata": {
        "id": "OmOUZqoMMaqf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. **How do we handle categorical variables in Machine Learning? What are the common techniques?**\n",
        "\n",
        "Categorical variables represent discrete groups or labels. Most machine learning algorithms cannot work directly with categorical data, so we need to convert them into numerical form.\n",
        "\n",
        "Common Techniques to Handle Categorical Variables:\n",
        "\n",
        "Label Encoding:\n",
        "\n",
        "Assigns a unique integer to each category.\n",
        "\n",
        "Example:\n",
        "\n",
        "Gender: Male ‚Üí 0, Female ‚Üí 1\n",
        "\n",
        "Useful for ordinal data (categories with an order).\n",
        "\n",
        "One-Hot Encoding:\n",
        "\n",
        "Converts each category into a binary column (0 or 1).\n",
        "\n",
        "Example:\n",
        "\n",
        "Color: Red, Green, Blue ‚Üí\n",
        "\n",
        "Red ‚Üí [1, 0, 0]\n",
        "\n",
        "Green ‚Üí [0, 1, 0]\n",
        "\n",
        "Blue ‚Üí [0, 0, 1]\n",
        "\n",
        "Useful for nominal data (no specific order).\n",
        "\n",
        "Binary Encoding:\n",
        "\n",
        "Converts categories into binary numbers and represents them in columns.\n",
        "\n",
        "Useful when there are many categories (high cardinality).\n",
        "\n",
        "Target Encoding (Mean Encoding):\n",
        "\n",
        "Replace each category with the mean of the target variable for that category.\n",
        "\n",
        "Often used in regression problems.\n",
        "\n",
        "Frequency Encoding:\n",
        "\n",
        "Replace each category with its frequency/count in the dataset.\n",
        "\n",
        "Helps algorithms understand importance/popularity of categories.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "ykErmn7kNFqo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. **What do you mean by training and testing a dataset?**\n",
        " - In machine learning, a dataset is usually divided into two parts: training set and testing set. This helps in building a model that learns patterns and can generalize well to new data.\n",
        "\n",
        " 1. Training Dataset:\n",
        "\n",
        "The part of the dataset used to train the machine learning model.\n",
        "\n",
        "The model learns patterns, relationships, and parameters from this data.\n",
        "\n",
        "Example: If we have 1000 data points, 70% (700 points) may be used for training.\n",
        "\n",
        "2. Testing Dataset:\n",
        "\n",
        "The part of the dataset used to evaluate the performance of the trained model.\n",
        "\n",
        "The model has never seen this data before, so it helps check if the model can generalize to new, unseen data.\n",
        "\n",
        "Example: Remaining 30% (300 points) of the data is used for testing.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "M47ES120NeZq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. **What is sklearn.preprocessing?**\n",
        "sklearn.preprocessing is a module in Scikit-learn (Python library) that provides tools to prepare and transform data before feeding it into machine learning models. Proper preprocessing helps models learn better and perform efficiently.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "LL816VM5NyfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. **What is a Test set?**\n",
        "- A test set is a portion of a dataset that is kept separate from the training data and is used to evaluate the performance of a trained machine learning model.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "2c2nzf7tOBS3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. **How do we split data for model fitting (training and testing) in Python?\n",
        "How do you approach a Machine Learning problem?**\n",
        "\n",
        "- In machine learning, data is usually split into training set and testing set so that the model can learn from one part and be evaluated on the other.\n",
        "\n",
        "**Common Steps:**\n",
        "\n",
        "Use the train_test_split function from sklearn.model_selection.\n",
        "\n",
        "Decide the proportion of data for training and testing (commonly 70%-80% for training and 20%-30% for testing).\n",
        "\n",
        "Shuffle the data before splitting to ensure randomness\n",
        "\n",
        "\n",
        "\n",
        "**Approaching a machine learning problem generally involves the following steps**:\n",
        "\n",
        "***Define the Problem:***\n",
        "\n",
        "Understand the goal (classification, regression, clustering).\n",
        "\n",
        "Example: Predict house prices (regression) or detect spam emails (classification).\n",
        "\n",
        "**Collect Data:**\n",
        "\n",
        "Gather relevant datasets from sources like CSV files, databases, or APIs.\n",
        "\n",
        "**Explore & Understand Data (EDA):**\n",
        "\n",
        "Analyze the data using statistics and visualization.\n",
        "\n",
        "Check for missing values, outliers, and feature distributions.\n",
        "\n",
        "**Preprocess Data:**\n",
        "\n",
        "Handle missing values\n",
        "\n",
        "Encode categorical variables\n",
        "\n",
        "Scale or normalize features\n",
        "\n",
        "**Split Data:**\n",
        "\n",
        "Divide the data into training set and testing set.\n",
        "\n",
        "**Choose a Model:**\n",
        "\n",
        "Select an appropriate algorithm (e.g., Linear Regression, Decision Tree, SVM).\n",
        "\n",
        "**Train the Model:**\n",
        "\n",
        "Fit the model on the training data.\n",
        "\n",
        "**Evaluate the Model:**\n",
        "\n",
        "Use the test set and metrics like accuracy, MSE, F1-score to check performance.\n",
        "\n",
        "**Tune Hyperparameters:**\n",
        "\n",
        "Adjust model parameters to improve performance.\n",
        "\n",
        "**Deploy & Monitor:**\n",
        "\n",
        "Use the trained model in real-world applications.\n",
        "\n",
        "Monitor performance and retrain if necessary.\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "aYy7oO0AONs-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. **Why do we have to perform EDA before fitting a model to the data?**\n",
        "Exploratory Data Analysis (EDA) is the process of analyzing and visualizing data to understand its structure, patterns, and quality before applying machine learning models.\n",
        "\n",
        "**Reasons to perform EDA:**\n",
        "\n",
        "\n",
        "**Understand the Data:**\n",
        "\n",
        "Identify the types of variables (continuous, categorical).\n",
        "\n",
        "Discover relationships and correlations between features.\n",
        "\n",
        "**Detect Missing Values:**\n",
        "\n",
        "Find and handle missing or null values that can affect model performance.\n",
        "\n",
        "**Identify Outliers:**\n",
        "\n",
        "Detect unusual data points that may distort the model.\n",
        "\n",
        "**Check Feature Distribution:**\n",
        "\n",
        "Understand how data is spread (normal, skewed, uniform).\n",
        "\n",
        "Helps in deciding scaling or transformation techniques.\n",
        "\n",
        "**Select Important Features:**\n",
        "\n",
        "Identify which features are most relevant for prediction.\n",
        "\n",
        "Reduces model complexity and improves accuracy.\n",
        "\n",
        "**Avoid Garbage In, Garbage Out:**\n",
        "\n",
        "Ensures the model is trained on clean, meaningful, and relevant data, not raw unprocessed data.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "F8m5BFDSO8AE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. **What is correlation?**\n",
        " - Correlation is a statistical measure that describes the strength and direction of a relationship between two variables.\n",
        "\n",
        " ---"
      ],
      "metadata": {
        "id": "2jWxv87JPoxI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. **What does negative correlation mean?**\n",
        " - Negative correlation occurs when two variables move in opposite directions.\n",
        "\n",
        " ---"
      ],
      "metadata": {
        "id": "WKakgimmPw1Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. **How can you find correlation between variables in Python?**\n",
        "- In Python, we can find correlation between variables using libraries like Pandas or NumPy. Correlation shows the strength and direction of a relationship between two variables.\n",
        "\n",
        "**Common Methods:**\n",
        "\n",
        "**Using Pandas corr() function:**\n",
        "\n",
        "df.corr() calculates correlation between all numerical columns in a DataFrame.\n",
        "\n",
        "Returns a correlation matrix showing correlation coefficients.\n",
        "\n",
        "**Using NumPy corrcoef() function:**\n",
        "\n",
        "np.corrcoef(x, y) calculates correlation between two arrays/lists.\n",
        "\n",
        "Returns the correlation coefficient (r value).\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "b2qVQIxNQAwY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. **What is causation? Explain difference between correlation and causation with an example.**\n",
        "- Causation means that one variable directly affects or causes a change in another variable.\n",
        "\n",
        "It shows a cause-and-effect relationship.\n",
        "\n",
        "Example: Smoking causes lung cancer ‚Üí Smoking (cause) directly increases the risk of lung cancer (effect).\n",
        "\n",
        "| Feature      | Correlation                                                             | Causation                                                         |\n",
        "| ------------ | ----------------------------------------------------------------------- | ----------------------------------------------------------------- |\n",
        "| Meaning      | Measures how two variables **move together**                            | Shows that **one variable causes a change** in another            |\n",
        "| Relationship | Variables may move together **without causing each other**              | There is a **direct cause-and-effect** relationship               |\n",
        "| Value        | Correlation coefficient (r) indicates strength and direction            | No coefficient; it‚Äôs about **direct effect**                      |\n",
        "| Example      | Ice cream sales and drowning cases are correlated (both rise in summer) | Smoking causes lung cancer (smoking directly affects cancer risk) |\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "bLpf6NhOQVzf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. **What is an Optimizer? What are different types of optimizers? Explain each with an example**\n",
        "- An optimizer is an algorithm used to update the parameters (weights and biases) of a machine learning model during training in order to minimize the loss function.\n",
        "\n",
        "The goal of an optimizer is to help the model learn efficiently and reach the best possible performance.\n",
        "\n",
        "| Optimizer                             | Description                                                                                                          | Example                                                                                    |\n",
        "| ------------------------------------- | -------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------ |\n",
        "| **Gradient Descent (GD)**             | Updates all parameters using the gradient of the loss function w\\.r.t all training data.                             | Linear regression model: weights are updated after computing gradient over entire dataset. |\n",
        "| **Stochastic Gradient Descent (SGD)** | Updates parameters using **one training sample at a time**. Faster and can escape local minima.                      | Training a neural network where weights are updated after each data point.                 |\n",
        "| **Mini-Batch Gradient Descent**       | Combines GD and SGD: updates parameters using a **small batch of data**.                                             | Neural networks with batch size 32 or 64 for training.                                     |\n",
        "| **Adam (Adaptive Moment Estimation)** | Combines **Momentum and RMSProp**. Maintains adaptive learning rates for each parameter and accelerates convergence. | Deep learning models in TensorFlow/Keras or PyTorch.                                       |\n",
        "| **RMSProp**                           | Maintains **adaptive learning rate** for each parameter based on recent gradients. Prevents oscillations.            | Training RNNs where gradients can vanish or explode.                                       |\n",
        "| **Momentum**                          | Accelerates GD by adding a **fraction of the previous update** to the current update. Helps escape local minima.     | Training deep neural networks where convergence is slow.                                   |\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "_Vvt3UzLQuoE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. **What is sklearn.linear_model ?**\n",
        "- sklearn.linear_model is a module in Scikit-learn (Python library) that provides algorithms for linear models.\n",
        "\n",
        "Linear models are used to predict a target variable based on one or more input features.\n",
        "\n",
        "The module contains regression and classification algorithms that assume a linear relationship between input features and output.\n",
        "\n",
        "| Algorithm                        | Type                        | Description                                                                     | Example Use                                           |\n",
        "| -------------------------------- | --------------------------- | ------------------------------------------------------------------------------- | ----------------------------------------------------- |\n",
        "| **LinearRegression**             | Regression                  | Predicts a continuous target variable using a linear equation.                  | Predict house prices based on size, location, etc.    |\n",
        "| **LogisticRegression**           | Classification              | Predicts a binary or multi-class outcome using a logistic function.             | Spam email detection (spam or not spam)               |\n",
        "| **Ridge Regression**             | Regression                  | Linear regression with L2 regularization to prevent overfitting.                | Predicting stock prices with many correlated features |\n",
        "| **Lasso Regression**             | Regression                  | Linear regression with L1 regularization, performs feature selection.           | Predicting medical costs with many features           |\n",
        "| **ElasticNet**                   | Regression                  | Combines L1 and L2 regularization for balanced feature selection and shrinkage. | Predicting insurance claims                           |\n",
        "| **SGDClassifier / SGDRegressor** | Classification / Regression | Uses stochastic gradient descent for linear models.                             | Large datasets where batch gradient descent is slow   |\n",
        "\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "72OQ6Gg7RAGa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. **What does model.fit() do? What arguments must be given?**\n",
        "- In Scikit-learn (Python), model.fit() is the method used to train a machine learning model on a dataset.\n",
        "\n",
        "It makes the model learn patterns and relationships between input features and target variables.\n",
        "\n",
        "During fit(), the model adjusts its parameters (weights, biases, etc.) to minimize the loss function.\n",
        "\n",
        "**Arguments for model.fit()**\n",
        "\n",
        "**The typical arguments are:**\n",
        "\n",
        "**X (Features / Input Data):**\n",
        "\n",
        "A 2D array or DataFrame containing the input variables.\n",
        "\n",
        "Example: Columns like height, weight, age.\n",
        "\n",
        "**y (Target / Output Data):**\n",
        "\n",
        "A 1D array, Series, or DataFrame containing the target variable.\n",
        "\n",
        "Example: marks scored, house price, spam/not spam.\n",
        "\n",
        "**Optional Arguments:**\n",
        "\n",
        "Some models accept additional arguments like sample weights, number of iterations, regularization parameters, depending on the algorithm.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "JkuWZzn9RNrx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. **What does model.predict() do? What arguments must be given?**\n",
        "- In Scikit-learn (Python), model.predict() is the method used to make predictions using a trained machine learning model.\n",
        "\n",
        "After the model has been trained using model.fit(), it can predict outputs for new, unseen input data.\n",
        "\n",
        "It uses the patterns learned during training to generate predictions.\n",
        "\n",
        "\n",
        "**X (Features / Input Data):**\n",
        "\n",
        "A 2D array or DataFrame containing the new input variables for which predictions are needed.\n",
        "\n",
        "The number of columns/features must match the features used during training.\n",
        "\n",
        "**Optional Arguments:**\n",
        "\n",
        "Most Scikit-learn models only require X.\n",
        "\n",
        "Some advanced models may have additional optional arguments, but these are rare.\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "89bDQTjeRrpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. **What are continuous and categorical variables ?**\n",
        "\n",
        "\n",
        "**1. Continuous Variables:**\n",
        "\n",
        "Continuous variables are numerical variables that can take any value within a range.\n",
        "\n",
        "They are measurable and often include fractions or decimals.\n",
        "\n",
        "Examples:\n",
        "\n",
        "Height of a person (e.g., 170.5 cm)\n",
        "\n",
        "Temperature (e.g., 36.6¬∞C)\n",
        "\n",
        "Salary (e.g., 45000.75 INR)\n",
        "\n",
        "**2. Categorical Variables:**\n",
        "\n",
        "Categorical variables represent distinct categories or groups.\n",
        "\n",
        "They are qualitative and usually cannot be measured numerically.\n",
        "\n",
        "Examples:\n",
        "\n",
        "Gender (Male, Female, Other)\n",
        "\n",
        "Blood Group (A, B, AB, O)\n",
        "\n",
        "Product Category (Electronics, Clothing, Furniture)\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "mB0N5mcsR_rr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. **What is feature scaling? How does it help in Machine Learning?**\n",
        "\n",
        "- Feature scaling is a technique used to normalize or standardize the range of independent variables (features) in a dataset.\n",
        "\n",
        "Machine learning algorithms often perform better when all features are on a similar scale.\n",
        "\n",
        "Without scaling, features with larger numerical values can dominate the learning process.\n",
        "\n",
        "Common Methods of Feature Scaling:\n",
        "\n",
        "Min-Max Scaling (Normalization):\n",
        "\n",
        "Scales features to a fixed range, usually 0 to 1.\n",
        "\n",
        "Formula:\n",
        "\n",
        "ùëã\n",
        "ùë†\n",
        "ùëê\n",
        "ùëé\n",
        "ùëô\n",
        "ùëí\n",
        "ùëë\n",
        "=\n",
        "ùëã\n",
        "‚àí\n",
        "ùëã\n",
        "ùëö\n",
        "ùëñ\n",
        "ùëõ\n",
        "ùëã\n",
        "ùëö\n",
        "ùëé\n",
        "ùë•\n",
        "‚àí\n",
        "ùëã\n",
        "ùëö\n",
        "ùëñ\n",
        "ùëõ\n",
        "X\n",
        "scaled\n",
        "\t‚Äã\n",
        "\n",
        "=\n",
        "X\n",
        "max\n",
        "\t‚Äã\n",
        "\n",
        "‚àíX\n",
        "min\n",
        "\t‚Äã\n",
        "\n",
        "X‚àíX\n",
        "min\n",
        "\t‚Äã\n",
        "\n",
        "\t‚Äã\n",
        "\n",
        "\n",
        "Standardization (Z-score Scaling):\n",
        "\n",
        "Centers the data around mean = 0 and standard deviation = 1.\n",
        "\n",
        "Formula:\n",
        "\n",
        "ùëã\n",
        "ùë†\n",
        "ùëê\n",
        "ùëé\n",
        "ùëô\n",
        "ùëí\n",
        "ùëë\n",
        "=\n",
        "ùëã\n",
        "‚àí\n",
        "ùúá\n",
        "ùúé\n",
        "X\n",
        "scaled\n",
        "\t‚Äã\n",
        "\n",
        "=\n",
        "œÉ\n",
        "X‚àíŒº\n",
        "\n",
        "---\n",
        "\t‚Äã\n"
      ],
      "metadata": {
        "id": "bpikLex-ScQG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. **How do we perform scaling in Python?**\n",
        "- In Python, we usually perform feature scaling using Scikit-learn‚Äôs preprocessing module.\n",
        "\n",
        "Min-Max Scaling (Normalization):\n",
        "\n",
        "Use MinMaxScaler from sklearn.preprocessing\n",
        "\n",
        "Scales features to a fixed range (0 to 1)\n",
        "\n",
        "Standardization (Z-score Scaling):\n",
        "\n",
        "Use StandardScaler from sklearn.preprocessing\n",
        "\n",
        "Centers features around mean = 0 and standard deviation = 1\n",
        "\n",
        "Max Abs Scaling:\n",
        "\n",
        "Use MaxAbsScaler for data that is already centered at 0\n",
        "\n",
        "Scales features by their maximum absolute value\n",
        "\n",
        "Robust Scaling:\n",
        "\n",
        "Use RobustScaler to reduce the impact of outliers\n",
        "\n",
        "Centers using median and scales according to interquartile range (IQR)\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "GWREmBFdS0a7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. **What is sklearn.preprocessing?**\n",
        "- sklearn.preprocessing is a module in Scikit-learn (Python library) that provides tools to prepare and transform data before feeding it into machine learning models.\n",
        "\n",
        "Raw data often contains features with different scales, missing values, or categorical variables.\n",
        "\n",
        "sklearn.preprocessing helps to normalize, scale, encode, and transform such data.\n",
        "\n",
        "Proper preprocessing helps models learn better and perform efficiently.\n",
        "\n",
        "\n",
        "\n",
        "| Function / Class | Purpose                                                  |\n",
        "| ---------------- | -------------------------------------------------------- |\n",
        "| `StandardScaler` | Standardizes features to mean = 0, std = 1               |\n",
        "| `MinMaxScaler`   | Scales features to a fixed range (0 to 1)                |\n",
        "| `RobustScaler`   | Scales using median and IQR to reduce outlier impact     |\n",
        "| `Normalizer`     | Scales individual samples to unit norm                   |\n",
        "| `LabelEncoder`   | Converts categorical labels to integers                  |\n",
        "| `OneHotEncoder`  | Converts categorical features to binary columns          |\n",
        "| `Binarizer`      | Converts numerical values into binary based on threshold |\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "2nhF5E6RTJ-q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. **How do we split data for model fitting (training and testing) in Python?**\n",
        "\n",
        "- In machine learning, it is important to split the dataset into training and testing sets so that the model can learn patterns and be evaluated on unseen data.\n",
        "\n",
        "Use the train_test_split function from sklearn.model_selection.\n",
        "\n",
        "Specify the proportion of data for training and testing:\n",
        "\n",
        "Common split: 70%-80% for training, 20%-30% for testing.\n",
        "\n",
        "Shuffle the data before splitting to ensure random distribution of samples.\n",
        "\n",
        "Assign the split datasets to variables:\n",
        "\n",
        "X_train, X_test ‚Üí Features for training and testing\n",
        "\n",
        "y_train, y_test ‚Üí Target labels for training and testing\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "TP9HS1GOThqa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. **Explain data encoding?**\n",
        "\n",
        "- Data encoding is the process of converting categorical (non-numerical) data into numerical format so that machine learning algorithms can process it.\n",
        "\n",
        "Most ML algorithms can only work with numerical data, so encoding is essential.\n",
        "\n",
        "Encoding preserves the information in categorical variables while making it usable for models.\n",
        "\n",
        "| Technique                  | Description                                          | Example                                                |\n",
        "| -------------------------- | ---------------------------------------------------- | ------------------------------------------------------ |\n",
        "| **Label Encoding**         | Assigns a unique integer to each category            | Gender: Male ‚Üí 0, Female ‚Üí 1                           |\n",
        "| **One-Hot Encoding**       | Creates binary columns for each category             | Color: Red, Green, Blue ‚Üí \\[1,0,0], \\[0,1,0], \\[0,0,1] |\n",
        "| **Binary Encoding**        | Converts categories to binary numbers                | Category A ‚Üí 01, B ‚Üí 10, C ‚Üí 11                        |\n",
        "| **Target / Mean Encoding** | Replaces category with mean of target variable       | Product Type ‚Üí Average Sales per Product Type          |\n",
        "| **Frequency Encoding**     | Replaces category with occurrence count or frequency | City ‚Üí Number of times it appears in dataset           |\n",
        "\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "x9uWoo98Ty1l"
      }
    }
  ]
}